{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook stuck?\n",
    "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download scenes using python3 projects/AriaSyntheticEnvironment/aria_synthetic_environments_downloader.py --set train --scene-ids 0-9 --cdn-file aria_synthetic_environments_dataset_download_urls.json --output-dir projectaria_tools_ase_data --unzip True\n",
    "# conda create -n aria python=3.10 \n",
    "# pip install -r requirements.txt\n",
    "# conda activate aria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "DATASET_ROOT = \"/Users/sahithyaravi/Documents/projectaria_tools/projectaria_tools_ase_data\"  # Specify your own dataset path\n",
    "SCENE_ID = 1  # Select a scene id\n",
    "\n",
    "dataset_path = Path(DATASET_ROOT)\n",
    "print(\"Chosen ASE data path: \", dataset_path)\n",
    "print(f\"Using Scene {SCENE_ID} for these examples\")\n",
    "\n",
    "scene_path = dataset_path / str(SCENE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_snippets.readers import read_points_file, read_trajectory_file, read_language_file\n",
    "\n",
    "# Load scene point cloud using read_points_file()\n",
    "points_path = scene_path / \"semidense_points.csv.gz\"\n",
    "points = read_points_file(points_path)\n",
    "\n",
    "# Load a trajectory using read_trajectory_file() \n",
    "trajectory_path = scene_path / \"trajectory.csv\"\n",
    "trajectory = read_trajectory_file(trajectory_path)\n",
    "\n",
    "# Load a scene command language using read_language_file()\n",
    "language_path = scene_path / \"ase_scene_language.txt\"\n",
    "entities = read_language_file(language_path)\n",
    "\n",
    "from code_snippets.interpreter import language_to_bboxes\n",
    "\n",
    "# Interpret scene commands into 3D Boxes\n",
    "entity_boxes = language_to_bboxes(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/sahithyaravi/Documents/projectaria_tools\")\n",
    "from projectaria_tools.projects import ase\n",
    "\n",
    "def transform_3d_points(transform, points):\n",
    "    N = len(points)\n",
    "    points_h = np.concatenate([points, np.ones((N, 1))], axis=1)\n",
    "    transformed_points_h = (transform @ points_h.T).T\n",
    "    transformed_points = transformed_points_h[:, :-1]\n",
    "    return transformed_points\n",
    "    \n",
    "# Load camera calibration\n",
    "device = ase.get_ase_rgb_calibration()\n",
    "\n",
    "# Load the trajectory using read_trajectory_file() \n",
    "trajectory_path = scene_path / \"trajectory.csv\"\n",
    "trajectory = read_trajectory_file(trajectory_path)\n",
    "\n",
    "# Load scene point cloud using read_points_file()\n",
    "points_path = scene_path / \"semidense_points.csv.gz\"\n",
    "points_world = read_points_file(points_path)\n",
    "\n",
    "# Transform the points into the device coordinate frame\n",
    "T_world_from_device = trajectory[\"Ts_world_from_device\"][frame_idx]\n",
    "T_device_from_world = np.linalg.inv(T_world_from_device)\n",
    "points_device = transform_3d_points(T_device_from_world, points_world)\n",
    "\n",
    "# Transform the points into the RGB camera coordinate frame\n",
    "T_device_from_camera = device.get_transform_device_camera().to_matrix()\n",
    "T_camera_from_device = np.linalg.inv(T_device_from_camera)\n",
    "points_device = transform_3d_points(T_camera_from_device, points_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Bird Eye View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from projectaria_tools.core.sophus import SE3\n",
    "from projectaria_tools.core import calibration\n",
    "from projectaria_tools.utils.rerun_helpers import ToTransform3D\n",
    "from projectaria_tools.core import data_provider, calibration\n",
    "from projectaria_tools.core.image import InterpolationMethod\n",
    "\n",
    "def rotate_se3_about_forward_axis(T_scene_camera, theta):\n",
    "    \"\"\"\n",
    "    Rotates an SE3 matrix about its forward (z) axis by an angle theta.\n",
    "\n",
    "    Parameters:\n",
    "    T_scene_camera (numpy.ndarray): The original 4x4 SE3 matrix.\n",
    "    theta (float): The angle by which to rotate about the z-axis (in radians).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The rotated 4x4 SE3 matrix.\n",
    "    \"\"\"\n",
    "    # Extract the rotation part (3x3) from the SE3 matrix\n",
    "    R_scene_camera = T_scene_camera.rotation().to_matrix()\n",
    "    \n",
    "    # Extract the translation part (3x1) from the SE3 matrix\n",
    "    t_scene_camera = T_scene_camera.translation()\n",
    "    \n",
    "    # Create the rotation matrix for rotation about the z-axis\n",
    "    R_z = R.from_euler('z', theta).as_matrix()\n",
    "    \n",
    "    # Apply the rotation to the original rotation part\n",
    "    R_scene_camera_rotated = R_scene_camera @ R_z\n",
    "    \n",
    "    # Combine the rotated rotation part with the original translation part\n",
    "    T_scene_camera_rotated = np.eye(4)\n",
    "    T_scene_camera_rotated[:3, :3] = R_scene_camera_rotated\n",
    "    T_scene_camera_rotated[:3, 3] = t_scene_camera\n",
    "    \n",
    "    return SE3.from_matrix(T_scene_camera_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "def undistort_image(file_path, device, is_depth=False):\n",
    "    \n",
    "    if is_depth:\n",
    "        raw_image = np.array(Image.open(file_path)).astype(np.float32)\n",
    "    else:\n",
    "        raw_image = Image.open(file_path)\n",
    "    rectified_array = calibration.distort_by_calibration(raw_image, device, device, InterpolationMethod.BILINEAR)\n",
    "    return rectified_array\n",
    "#------------------------------\n",
    "def render_topdown_from_projected(points_world, colors, grid_resolution=0.01):\n",
    "    x, y, z = points_world[:, 0], points_world[:, 1], points_world[:, 2]\n",
    "    valid = (z > 0.1) & (z < 2.5)\n",
    "    x, y, colors = x[valid], y[valid], colors[valid]\n",
    "\n",
    "    min_x, min_y = x.min(), y.min()\n",
    "    max_x, max_y = x.max(), y.max()\n",
    "\n",
    "    W = int(np.ceil((max_x - min_x) / grid_resolution))\n",
    "    H = int(np.ceil((max_y - min_y) / grid_resolution))\n",
    "    img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "\n",
    "    xi = ((x - min_x) / grid_resolution).astype(int)\n",
    "    yi = ((y - min_y) / grid_resolution).astype(int)\n",
    "\n",
    "    img[H - yi - 1, xi] = colors\n",
    "    return img\n",
    "\n",
    "all_points = []\n",
    "all_colors = []\n",
    "\n",
    "T_Device_Cam = device.get_transform_device_camera()\n",
    "image_size = device.get_image_size()\n",
    "width, height = image_size[0], image_size[1]\n",
    "\n",
    "rays = np.empty((height, width, 3))\n",
    "for u in range(width):\n",
    "    for v in range(height):\n",
    "        ray = device.unproject([u, v])\n",
    "        if ray is not None:\n",
    "            ray = ray / np.linalg.norm(ray)\n",
    "        rays[v, u] = ray\n",
    "\n",
    "for frame_idx in range(0, num_frames, 10):\n",
    "    frame_id = str(frame_idx).zfill(7)\n",
    "    rgb_path = rgb_dir / f\"vignette{frame_id}.jpg\"\n",
    "    depth_path = depth_dir / f\"depth{frame_id}.png\"\n",
    "    if not rgb_path.exists() or not depth_path.exists():\n",
    "        continue\n",
    "\n",
    "    rgb = undistort_image(rgb_path, device, is_depth=False)\n",
    "    depth = undistort_image(depth_path, device, is_depth=True)\n",
    "    instances = undistort_image(instance_path, device)\n",
    "    instance_array = np.array(instances)\n",
    "    unique_instance_ids = np.unique(instance_array)\n",
    "\n",
    "    rgb = np.rot90(np.array(rgb), 3)\n",
    "    depth = np.rot90(np.array(depth).astype(np.float32), 3)\n",
    "    instance_array = np.rot90(instance_array, 3)\n",
    "    \n",
    "    T_Scene_Device = trajectory[\"Ts_world_from_device\"][frame_idx]\n",
    "    T_Device_Cam_rot = rotate_se3_about_forward_axis(T_Device_Cam, 3 * np.pi / 2)\n",
    "    T_Scene_Cam = T_Scene_Device @ T_Device_Cam_rot.to_matrix()\n",
    "\n",
    "    valid_mask = (rays is not None) & (depth > 0)\n",
    "    indices = np.argwhere(valid_mask)\n",
    "    u_indices, v_indices = indices[:, 1], indices[:, 0]\n",
    "    rays_selected = rays[v_indices, u_indices]\n",
    "    depth_selected = depth[v_indices, u_indices] / 1000.0\n",
    "\n",
    "    p_in_cam = (depth_selected[:, None] * rays_selected)\n",
    "    p_in_scene = transform_3d_points(T_Scene_Cam, p_in_cam)\n",
    "\n",
    "    colors = rgb[v_indices, u_indices]\n",
    "    all_points.append(p_in_scene)\n",
    "    all_colors.append(colors)\n",
    "\n",
    "points_world = np.concatenate(all_points, axis=0)\n",
    "colors = np.concatenate(all_colors, axis=0)\n",
    "rgb_numpy = np.asarray(rgb)\n",
    "instance_ids = instance_array[v_indices, u_indices]\n",
    "\n",
    "\n",
    "topdown_img = render_topdown_from_projected(points_world, colors)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(topdown_img)\n",
    "plt.title(\"Top-Down RGB Map (ARIA Accurate)\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"topdown_rgb_map_corrected.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 3D Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each modality together in 3D\n",
    "from code_snippets.plotters import plot_point_cloud, plot_trajectory, plot_box_wireframe\n",
    "\n",
    "plot_traces = []\n",
    "# Create a trace for the pointcloud using plot_point_cloud()\n",
    "plot_traces.append(plot_point_cloud(points))\n",
    "# Create a trace for the trajectory using plot_trajectory()\n",
    "plot_traces.append(plot_trajectory(trajectory))\n",
    "\n",
    "# Create a trace for each entity box in the form of a wireframe using plot_box_wireframe()\n",
    "for entity_box in entity_boxes:\n",
    "    plot_traces.append(plot_box_wireframe(entity_box))\n",
    "\n",
    "fig = go.Figure(data=plot_traces)\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    scene={\n",
    "        \"xaxis\": {\"showticklabels\": False, \"title\": \"\"},\n",
    "        \"yaxis\": {\"showticklabels\": False, \"title\": \"\"},\n",
    "        \"zaxis\": {\"showticklabels\": False, \"title\": \"\"},\n",
    "    },\n",
    ")\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we include a plotting function that will handle all of the steps for you.\n",
    "from code_snippets.plotters import plot_3d_scene\n",
    "\n",
    "plot_3d_scene(\n",
    "    language_path=language_path,\n",
    "    points_path=points_path,\n",
    "    trajectory_path=trajectory_path\n",
    ")\n",
    "# save as a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Top-down\n",
    "# fig.update_layout(scene_camera=dict(eye=dict(x=0., y=0., z=2.)))\n",
    "# fig.write_image(\"top_view.png\")\n",
    "\n",
    "# # Front view\n",
    "# fig.update_layout(scene_camera=dict(eye=dict(x=0., y=2., z=0.)))\n",
    "# fig.write_image(\"front_view.png\")\n",
    "\n",
    "# # Side view\n",
    "# fig.update_layout(scene_camera=dict(eye=dict(x=2., y=0., z=0.)))\n",
    "# fig.write_image(\"side_view.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Loading and Plotting Images and Image Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bright_colormap(num_colors=1024):\n",
    "    bright_colors = np.random.rand(num_colors, 3)\n",
    "    bright_colors = (bright_colors + 1) / 2\n",
    "    return colors.ListedColormap([c for c in bright_colors])\n",
    "\n",
    "scene_path = dataset_path / str(SCENE_ID)\n",
    "\n",
    "rgb_dir = scene_path / \"rgb\"\n",
    "depth_dir = scene_path / \"depth\"\n",
    "instance_dir = scene_path / \"instances\"\n",
    "\n",
    "# Choose a random frame to plot from the scene's images\n",
    "num_frames = len(list(rgb_dir.glob(\"*.jpg\")))\n",
    "frame_idx = np.random.randint(num_frames)\n",
    "\n",
    "# Load images\n",
    "frame_id = str(frame_idx).zfill(7)\n",
    "\n",
    "rgb_path = rgb_dir / f\"vignette{frame_id}.jpg\"\n",
    "depth_path = depth_dir / f\"depth{frame_id}.png\"\n",
    "instance_path = instance_dir / f\"instance{frame_id}.png\"\n",
    "\n",
    "rgb = Image.open(rgb_path)\n",
    "depth = Image.open(depth_path)\n",
    "instances = Image.open(instance_path)\n",
    "\n",
    "# Note: Images are rotated to upright for visualization.\n",
    "# However, the camera calibration is for the original orientation.\n",
    "rgb_to_plot = rgb.rotate(-90, expand=True)\n",
    "depth_to_plot = depth.rotate(-90, expand=True)\n",
    "instances_to_plot = instances.rotate(-90, expand=True)\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(15, 5), dpi=300)\n",
    "axes[0].imshow(rgb_to_plot)\n",
    "axes[0].set_title(\"RGB Image\")\n",
    "axes[1].imshow(np.array(depth_to_plot), cmap=\"plasma\")\n",
    "axes[1].set_title(\"Metric Depth (mm)\")\n",
    "axes[2].imshow(np.array(instances_to_plot), cmap=random_bright_colormap())\n",
    "axes[2].set_title(\"Instance Map\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Project Points into Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Choose and load random frame to plot from the scene's images\n",
    "num_frames = len(list(rgb_dir.glob(\"*.jpg\")))\n",
    "frame_idx = np.random.randint(num_frames)\n",
    "frame_id = str(frame_idx).zfill(7)\n",
    "rgb_path = rgb_dir / f\"vignette{frame_id}.jpg\"\n",
    "rgb = Image.open(rgb_path)\n",
    "\n",
    "\n",
    "# Project points into the image\n",
    "points_image = []\n",
    "depths = []\n",
    "for point_device in points_device:\n",
    "    point_image = device.project(point_device)\n",
    "    if point_image is not None:\n",
    "        points_image.append(point_image)\n",
    "points_image = np.stack(points_image)\n",
    "\n",
    "# Overlay projected points onto image\n",
    "plt.imshow(rgb)\n",
    "plt.scatter(points_image[:, 0], points_image[:, 1], s=0.01, alpha=0.3, c=\"#FFFFFF\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ase.get_ase_rgb_calibration())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aria",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
